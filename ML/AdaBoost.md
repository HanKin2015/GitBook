# AdaBoost算法

参考：
https://blog.csdn.net/qq_38890412/article/details/120360354

## 1、关于Boost
集成学习中有两个重要概念，分别为Bagging和Boost。其中Boost也被称为增强学习或提升法，是一种重要的集成学习方法，它能够将预测精度仅仅比随机猜测略高的弱学习器增强为预测精度很高的强学习器。这是在直接构造强学习器较为困难的情况下，为学习算法提供了一种有效的新思路和新方法。其中较为成功的是上个世纪90年代Yoav Freund和Robert Schapire提出的AdaBoost算法。
![Boost原理图](https://img-blog.csdnimg.cn/2021091809193281.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6LaK5p2l6LaK6IOW55qER3VhblJ1bndlaQ==,size_20,color_FFFFFF,t_70,g_se,x_16)
可以将上图过程总结为：
- 对原始数据集初始化权重
- 用带权值数据集训练弱学习器
- 根据弱学习器的误差计算弱学习器的权重
- 调整数据集的权重
- 重复第2-4步K-1次
- 将K-1个弱学习器的结果进行加权组合

## 2、AdaBoost概要介绍
Adaboost是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）。
AdaBoost是Adaptive Boosting（自适应增强）的缩写，它的自适应在于：被前一个基本分类器误分类的样本的权值会增大，而正确分类的样本的权值会减小，并再次用来训练下一个基本分类器。同时，在每一轮迭代中，加入一个新的弱分类器，直到达到某个预定的足够小的错误率或预先指定的最大迭代次数再确定最后的强分类器。

![](https://img-blog.csdnimg.cn/img_convert/42d304a6cba0f6db34256093de3f4cde.png)
从上图来看，AdaBoost算法可以简化为3个步骤：
首先，是初始化训练数据的权值分布D1。假设有N个训练样本数据，则每一个训练样本最开始时，都会被赋予相同的权值：w1 = 1/N。
训练弱分类器Ci。具体训练过程：如果某个训练样本点，被弱分类器Ci准确地分类，那么再构造下一个训练集中，它对应的权值要减小；相反，如果某个训练样本点被错误分类，那么它的权值就应该增大。权值的更新过的样本被用于训练下一个弱分类器，整个过程如此迭代下去。
最后，将各个训练得到的弱分类器组合成一个强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。
换而言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。

## 3、AdaBoost算法过程
给定训练数据集：（x1,y1）,(x2,y2)···(xn,yn)，其中yi属于{1，-1}用于表示训练样本的类别标签，i=1,...,N。Adaboost的目的就是从训练数据中学习一系列弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。

代码实现：https://blog.csdn.net/sgsgsgwe/article/details/125314311

## 4、实战

## 5、AdaBoost算法的优缺点
优点：
Adaboost作为分类器时，分类精度很高；
在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活；
作为简单的二元分类器时，构造简单，结果可理解；
不容易发生过拟合。

缺点：
对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。
