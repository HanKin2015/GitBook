# trick
https://www.zhihu.com/question/39278927

## 1、机器学习常说到的trick是什么意思
trick指得是技巧的意思，也就是可以使机器学习或者深度学习等在训练时提升性能的小技巧和方法（如深度学习中的数据增强等）

## 2、Trick 1：权值初始化
```
weights = np.ones(N)/N
```
应用场景：
（1）由一堆 decision stump 构成的 weak classifiers 用于AdaBoost 时的的初始权重分配；

## 3、Trick 2：避免分母为零的方法
令分母上可能的小量:
```
alpha = 1/2*np.log((1-epsilon)/max(epsilon, 1e-16))
```

## 4、为什么需要hash trick？
在工业界，数据经常不仅是量大，而且维度也很高，所以出现很多具体的大规模的机器学习问题，比如点击率预测问题。在CTR中，特征涉及到广告主和用户等。大多特征都可以看做categorical。对categorical feature一般使用1-of-c编码方式（统计里称为dummy coding）。对于取值为实数的特征我们可以进行离散化处理（实际应用中一般也不直接把连续值的特征直接交由模型处理）。可能有的特征对应的取值非常多，所以这种编码方式就容易导致维度非常高（维数灾难问题）。当然也有其他原因造成该问题维度很高，比如将不同特征做笛卡尔积产生新的特征（参考链接6中提到很多广告公司宣称使用的几十上百亿特征都是用这个方法搞出来的）。
那如何降维呢？我们可以首先去除特征中不频繁的值，这样特征对应的取值减少，维数会降低。但是这种方法需要对数据进行预处理。至于PCA等常见的降维方法，由于数据量实在太大而不太适合使用。而hash trick是一种越来越受欢迎的降维方法。它不需要进行数据预处理，实现简单直接。
对于一些非线性问题，我们可以将输入空间映射到高维的特征的空间，使问题变成一个线性问题。使用kernel trick可以通过低维度的样本点的核函数计算得到高维度中向量的内积。但是在文本分类问题中，就会碰到一个问题。文本分类问题也是一个典型的高维问题，而且由于手工添加的一些非线性特征，原始的输入空间就线性可分，这样就没必要在将输入映射到更高位的特征空间了。而是需要使用降维方法。hash trick和kernel trick的作用相反。

## 5、维度灾难
维度灾难是在数字图像处理中，对于已知样本数目，存在一个特征数目的最大值，当实际使用的特征数目超过这个最大值时，分类器的性能不是得到改善，而是退化。
这种现象正是在识别模式中被称为“维度灾难”的一种表现形式。此外，提取特征向量的维度过高会增加计算的复杂度，给后续的分类问题造成负面影响。



